{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import os\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor5_path = \"./shape_predictor_5_face_landmarks.dat\"\n",
    "predictor68_path = \"./shape_predictor_68_face_landmarks.dat\"\n",
    "face_rec_model_path = \"./dlib_face_recognition_resnet_model_v1.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp5 = dlib.shape_predictor(predictor68_path)\n",
    "sp68 = dlib.shape_predictor(predictor68_path)\n",
    "facerec = dlib.face_recognition_model_v1(face_rec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(im):\n",
    "    dets = detector(im, 1)\n",
    "    if len(dets) == 0:\n",
    "        return None  # no face detected :(\n",
    "    for d in dets:\n",
    "        # TODO: what if there are two faces in one pic --> seems ok\n",
    "#         print(d)\n",
    "        cur_feat = []\n",
    "        x_l = d.left()\n",
    "        x_r = d.right()\n",
    "        y_t = d.top()\n",
    "        y_b = d.bottom()\n",
    "\n",
    "\n",
    "        width = x_r - x_l\n",
    "        height = y_b - y_t\n",
    "    #     cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 255, 0), im.shape[0]//150)\n",
    "        shape = sp68(im, d)\n",
    "        for i in range(0, 68):\n",
    "            # TODO: normalize to 0 mean? How?\n",
    "            feat_x = (shape.part(i).x - x_l) / width\n",
    "            feat_y = (shape.part(i).y - y_t) / height\n",
    "            cur_feat.append(feat_x)\n",
    "            cur_feat.append(feat_y)\n",
    "\n",
    "    #         cv2.circle(orig, (shape.part(i).x, shape.part(i).y), 1, (255, 0, 0), im.shape[0]//150)\n",
    "    #     feats.append(cur_feat)\n",
    "    return cur_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train  = feats\n",
    "y = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC() # TODO: consider non-linear kernels: svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-11cc0abebe76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr',\n\u001b[0;32m--> 227\u001b[0;31m                          dtype=np.float64, order=\"C\")\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    541\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    408\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "clf.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict([feats[1], feats[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"] #Emotion list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(emotion): #Define function to get file list, randomly shuffle it and split 80/20\n",
    "    files = glob.glob(\"google_dataset\\\\%s\\\\*\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "    return training, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "        for item in training:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            current_feats = get_features(gray)\n",
    "            if current_feats:\n",
    "                training_data.append(current_feats) #append image feats to training data list\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "    \n",
    "        for item in prediction: #repeat above process for prediction set\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            current_feats = get_features(gray)\n",
    "            if current_feats:\n",
    "                prediction_data.append(current_feats)\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "\n",
    "    return training_data, training_labels, prediction_data, prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(-44, 27) (312, 348)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(-44, 63) (312, 384)]\n",
      "[(-53, -52) (375, 376)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(-44, 27) (312, 348)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(-44, 27) (312, 348)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(-44, 27) (312, 348)]\n",
      "[(63, 27) (384, 348)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(63, 27) (384, 348)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(43, 93) (266, 316)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 82) (290, 350)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(-53, 33) (375, 418)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(68, 93) (291, 316)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(-44, 27) (312, 348)]\n",
      "[(27, 27) (348, 348)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(-44, 63) (312, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(22, 52) (290, 320)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(68, 93) (291, 316)]\n",
      "[(52, 52) (319, 320)]\n",
      "[(52, 82) (319, 350)]\n",
      "[(27, 63) (348, 384)]\n",
      "[(27, 63) (348, 384)]\n"
     ]
    }
   ],
   "source": [
    "sets_made = make_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_recognizer():\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "#     training_data, training_labels, prediction_data, prediction_labels = sets_made  # to speed up TEMP\n",
    "    \n",
    "    print (\"training SVM classifier\")\n",
    "    print (\"size of training set is:\", len(training_labels), \"images\")\n",
    "    print (\"size of test set is:\", len(prediction_labels), \"images\")\n",
    "    \n",
    "    X = training_data\n",
    "    y = training_labels\n",
    "    \n",
    "#     clf = svm.LinearSVC(C=5)\n",
    "    clf = svm.SVC(C=0.8, kernel='rbf', gamma=20)  # it's worse\n",
    "    clf.fit(X, y)\n",
    "    print (\"predicting classification set\")\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for image in training_data:\n",
    "        pred = clf.predict([image]) #predict emotion\n",
    "        if pred == training_labels[cnt]: #validate it\n",
    "            correct += 1\n",
    "            cnt += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            cnt += 1\n",
    "    print(\"Training accuracy:\", ((100*correct)/(correct + incorrect))) \n",
    "\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for image in prediction_data:\n",
    "        pred = clf.predict([image]) #predict emotion\n",
    "        if pred == prediction_labels[cnt]: #validate it\n",
    "            correct += 1\n",
    "            cnt += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            cnt += 1\n",
    "    return ((100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run_recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM classifier\n",
      "size of training set is: 132 images\n",
      "size of test set is: 30 images\n",
      "predicting classification set\n",
      "Training accuracy: 94.6969696969697\n",
      "Test: got 26.666666666666668 percent correct!\n",
      "training SVM classifier\n",
      "size of training set is: 130 images\n",
      "size of test set is: 31 images\n",
      "predicting classification set\n",
      "Training accuracy: 93.84615384615384\n",
      "Test: got 25.806451612903224 percent correct!\n",
      "training SVM classifier\n",
      "size of training set is: 131 images\n",
      "size of test set is: 31 images\n",
      "predicting classification set\n",
      "Training accuracy: 93.89312977099236\n",
      "Test: got 29.032258064516128 percent correct!\n",
      "\n",
      "\n",
      "end score: 27.1684587814 percent correct!\n"
     ]
    }
   ],
   "source": [
    "#Now run it\n",
    "metascore = []\n",
    "for i in range(0,3):\n",
    "    correct = run_recognizer()\n",
    "    print (\"Test: got\", correct, \"percent correct!\")\n",
    "    metascore.append(correct)\n",
    "\n",
    "print (\"\\n\\nend score:\", np.mean(metascore), \"percent correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}